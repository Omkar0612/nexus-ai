package agents

/*
HallucinationDetector â€” local self-check on every LLM response.

The #2 most-cited AI agent reliability problem (Feb 2026):
'The agent confidently told me the wrong API endpoint and I deployed it.'
â€” r/LocalLLaMA, multiple threads

NEXUS approach (no external API needed):
  1. Extract factual claims from the LLM response (dates, names, numbers, URLs)
  2. Cross-reference each claim against episodic memory for contradictions
  3. Score overall response confidence (0.0â€“1.0)
  4. Tag response: [VERIFIED], [UNVERIFIED], or [CONTRADICTED]
  5. Flag specific suspicious sentences for user review
  6. Optional: auto-retry with stricter prompt if confidence < threshold

Uses local heuristics + lightweight pattern matching.
Zero cost, zero external API calls, works offline.
*/

import (
	"fmt"
	"math"
	"regexp"
	"strings"
	"time"
)

// VerificationTag classifies an LLM response's reliability
type VerificationTag string

const (
	TagVerified     VerificationTag = "VERIFIED"
	TagUnverified   VerificationTag = "UNVERIFIED"
	TagContradicted VerificationTag = "CONTRADICTED"
	TagUncertain    VerificationTag = "UNCERTAIN"
)

// Claim is a single extracted factual assertion
type Claim struct {
	Text       string
	ClaimType  string  // date, number, url, name, assertion
	Confidence float64 // 0.0 = very suspicious, 1.0 = likely correct
	Flagged    bool
	Reason     string
}

// HallucinationReport is the full analysis of a response
type HallucinationReport struct {
	Response        string
	Claims          []Claim
	OverallScore    float64  // 0.0â€“1.0
	Tag             VerificationTag
	FlaggedCount    int
	Contradictions  []string
	Suggestion      string
	AnalysedAt      time.Time
	ShouldRetry     bool
	RetryPromptHint string
}

// HallucinationDetector analyses LLM responses for reliability
type HallucinationDetector struct {
	retryThreshold  float64 // score below this triggers retry suggestion
	memoryContext   []string // recent verified facts from memory
	urlPattern      *regexp.Regexp
	datePattern     *regexp.Regexp
	numberPattern   *regexp.Regexp
	hyperboleWords  []string
	hedgeWords      []string
	certaintyWords  []string
}

// NewHallucinationDetector creates a detector with default settings
func NewHallucinationDetector(retryThreshold float64) *HallucinationDetector {
	if retryThreshold <= 0 {
		retryThreshold = 0.6
	}
	return &HallucinationDetector{
		retryThreshold: retryThreshold,
		urlPattern:     regexp.MustCompile(`https?://[\w./?=#&%-]+`),
		datePattern:    regexp.MustCompile(`\b(\d{4}|Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\b`),
		numberPattern:  regexp.MustCompile(`\b\d+(?:[,.]\d+)?(?:\s*(?:million|billion|thousand|%|USD|\$))?\b`),
		hyperboleWords: []string{"always", "never", "every", "all", "none", "guaranteed", "certainly", "definitely", "100%", "impossible"},
		hedgeWords:     []string{"might", "may", "could", "possibly", "perhaps", "approximately", "around", "estimated", "roughly", "I think", "I believe"},
		certaintyWords: []string{"is", "are", "was", "were", "will", "the exact", "precisely"},
	}
}

// LoadMemoryContext loads verified facts from recent memory for cross-referencing
func (h *HallucinationDetector) LoadMemoryContext(facts []string) {
	h.memoryContext = facts
}

// Analyse runs hallucination detection on an LLM response
func (h *HallucinationDetector) Analyse(response string) *HallucinationReport {
	report := &HallucinationReport{
		Response:    response,
		AnalysedAt:  time.Now(),
	}

	claims := h.extractClaims(response)
	report.Claims = claims

	// Score each claim
	var totalScore float64
	for i := range claims {
		claims[i].Confidence = h.scoreClaim(&claims[i], response)
		if claims[i].Flagged {
			report.FlaggedCount++
		}
		totalScore += claims[i].Confidence
	}

	// Cross-reference against memory
	report.Contradictions = h.findContradictions(response)

	// Calculate overall score
	if len(claims) > 0 {
		report.OverallScore = totalScore / float64(len(claims))
	} else {
		// No specific claims = safe default
		report.OverallScore = 0.8
	}

	// Penalise for contradictions
	report.OverallScore -= float64(len(report.Contradictions)) * 0.15
	report.OverallScore = math.Max(0, math.Min(1, report.OverallScore))

	report.Tag = h.assignTag(report)
	report.ShouldRetry = report.OverallScore < h.retryThreshold
	if report.ShouldRetry {
		report.RetryPromptHint = "Please be more cautious. Only assert facts you are certain about. Qualify uncertain information with 'I believe' or 'approximately'."
	}
	report.Suggestion = h.buildSuggestion(report)
	return report
}

func (h *HallucinationDetector) extractClaims(text string) []Claim {
	var claims []Claim

	// Extract URLs
	for _, url := range h.urlPattern.FindAllString(text, -1) {
		claims = append(claims, Claim{Text: url, ClaimType: "url"})
	}

	// Extract date references
	for _, date := range h.datePattern.FindAllString(text, -1) {
		claims = append(claims, Claim{Text: date, ClaimType: "date"})
	}

	// Extract sentences with strong assertions
	sentences := strings.Split(text, ".")
	for _, s := range sentences {
		s = strings.TrimSpace(s)
		if len(s) < 20 {
			continue
		}
		lower := strings.ToLower(s)
		for _, hw := range h.hyperboleWords {
			if strings.Contains(lower, hw) {
				claims = append(claims, Claim{
					Text:      s,
					ClaimType: "assertion",
					Flagged:   true,
					Reason:    fmt.Sprintf("contains absolute term: '%s'", hw),
				})
				break
			}
		}
	}
	return claims
}

func (h *HallucinationDetector) scoreClaim(c *Claim, fullText string) float64 {
	score := 0.75 // baseline
	lower := strings.ToLower(c.Text)

	// Hedge words increase confidence (model is being appropriately uncertain)
	for _, hw := range h.hedgeWords {
		if strings.Contains(lower, hw) {
			score += 0.05
		}
	}

	// Hyperbole / absolute words decrease confidence
	for _, hw := range h.hyperboleWords {
		if strings.Contains(lower, hw) {
			score -= 0.15
			c.Flagged = true
			if c.Reason == "" {
				c.Reason = fmt.Sprintf("absolute language: '%s'", hw)
			}
		}
	}

	// URLs that look broken
	if c.ClaimType == "url" {
		if !strings.Contains(c.Text, ".") || strings.HasSuffix(c.Text, "/") {
			score -= 0.2
			c.Flagged = true
			c.Reason = "URL may be invalid"
		}
	}

	// Cross-reference memory context
	for _, fact := range h.memoryContext {
		if contradicts(c.Text, fact) {
			score -= 0.3
			c.Flagged = true
			c.Reason = fmt.Sprintf("contradicts memory: '%s'", truncate(fact, 60))
		}
	}

	return math.Max(0, math.Min(1, score))
}

func (h *HallucinationDetector) findContradictions(response string) []string {
	var contradictions []string
	for _, fact := range h.memoryContext {
		if contradicts(response, fact) {
			contradictions = append(contradictions, fmt.Sprintf("Response may contradict: \"%s\"", truncate(fact, 80)))
		}
	}
	return contradictions
}

func (h *HallucinationDetector) assignTag(r *HallucinationReport) VerificationTag {
	if len(r.Contradictions) > 0 {
		return TagContradicted
	}
	if r.OverallScore >= 0.75 {
		return TagVerified
	}
	if r.OverallScore >= 0.5 {
		return TagUncertain
	}
	return TagUnverified
}

func (h *HallucinationDetector) buildSuggestion(r *HallucinationReport) string {
	switch r.Tag {
	case TagVerified:
		return ""
	case TagContradicted:
		return "âš ï¸ This response may contradict previously known facts. Verify before acting."
	case TagUnverified:
		return "ðŸ¤” Low confidence response. Consider asking NEXUS to cite sources or try again."
	case TagUncertain:
		return "ðŸ’¡ Some claims could not be verified. Double-check key facts."
	}
	return ""
}

// FormatBadge returns a compact inline tag for a response
func (r *HallucinationReport) FormatBadge() string {
	switch r.Tag {
	case TagVerified:
		return fmt.Sprintf("âœ… VERIFIED (%.0f%%)", r.OverallScore*100)
	case TagUncertain:
		return fmt.Sprintf("ðŸ’¡ UNCERTAIN (%.0f%%)", r.OverallScore*100)
	case TagUnverified:
		return fmt.Sprintf("âš ï¸ UNVERIFIED (%.0f%%)", r.OverallScore*100)
	case TagContradicted:
		return fmt.Sprintf("ðŸš¨ CONTRADICTED (%.0f%%) â€” %d contradiction(s)", r.OverallScore*100, len(r.Contradictions))
	}
	return ""
}

func contradicts(a, b string) bool {
	// Simple heuristic: look for shared named entities with conflicting numbers
	wordsA := strings.Fields(strings.ToLower(a))
	wordsB := strings.Fields(strings.ToLower(b))
	shared := 0
	for _, wa := range wordsA {
		if len(wa) > 5 {
			for _, wb := range wordsB {
				if wa == wb {
					shared++
				}
			}
		}
	}
	// Only flag if high word overlap but contains opposite signals
	if shared < 3 {
		return false
	}
	oppositePairs := [][2]string{
		{"increase", "decrease"}, {"up", "down"}, {"higher", "lower"},
		{"true", "false"}, {"yes", "no"}, {"enabled", "disabled"},
	}
	for _, pair := range oppositePairs {
		hasA := strings.Contains(strings.ToLower(a), pair[0]) && strings.Contains(strings.ToLower(b), pair[1])
		hasB := strings.Contains(strings.ToLower(a), pair[1]) && strings.Contains(strings.ToLower(b), pair[0])
		if hasA || hasB {
			return true
		}
	}
	return false
}
